{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import random as rd\n",
    "from enum import Enum\n",
    "class nodeType(Enum):\n",
    "    NODE_NULL = 1\n",
    "    NODE_LEAF = 2\n",
    "    NODE_INTERNAL = 3\n",
    "class AttributeType(Enum):\n",
    "    TYPE_NUMERICAL = 1 \n",
    "    TYPE_CATEGORICAL = 2\n",
    "\n",
    "def timerfunc(func):\n",
    "    \"\"\"\n",
    "    A timer decorator\n",
    "    \"\"\"\n",
    "    def function_timer(*args, **kwargs):\n",
    "        \"\"\"\n",
    "        A nested function for timing other functions\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        value = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        runtime = end - start\n",
    "        msg = \"The runtime for {func} took {time} seconds to complete\"\n",
    "        print(msg.format(func=func.__name__,\n",
    "                         time=runtime))\n",
    "        return value\n",
    "    return function_timer\n",
    "\n",
    "\n",
    "# w, h = 8, 5;\n",
    "# Matrix = [[0 for x in range(w)] for y in range(h)] \n",
    "# print (Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING RNG WITH SEED:  30\n",
      "----- DATASET [ CHESS_KING_ROOK_VERSUS_KING_PAWN ]\n",
      "----- NUMBER OF SAMPLES:  3196\n",
      "----- NUMBER OF ATTRIBUTES:  36\n",
      "----- NUMBER OF CLASSES:  2\n",
      "The runtime for Params took 0.2860865592956543 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# \"C:\\\\Users\\\\iannu\\\\Dropbox\\\\doutorado\\\\metaheuristics\\\\decision-tree\\\\Datasets\\\\p01.txt\"\n",
    "@timerfunc\n",
    "class Params:\n",
    "    def __init__(self, pathToInstance, pathToSolution, seedRNG, maxDepth, maxTime):\n",
    "        rd.seed(seedRNG)\n",
    "        print(\"INITIALIZING RNG WITH SEED: \",seedRNG)\n",
    "        f=open(pathToInstance, \"r\")\n",
    "\n",
    "        attributeTypes = []\n",
    "        \n",
    "        line = f.readline()        \n",
    "        datasetName = line.replace(\"\\n\",\"\").split(\" \")[1]\n",
    "        line = f.readline()\n",
    "        nbSamples = int(line.split(\" \")[1])\n",
    "        line = f.readline()\n",
    "        nbAttributes = int(line.split(\" \")[1])\n",
    "        line = f.readline()\n",
    "        attType = line.replace(\"\\n\",\"\").split(\" \")\n",
    "\n",
    "        line = f.readline()\n",
    "        nbClasses = int(line.split(\" \")[1])\n",
    "        attributeTypes = []\n",
    "        for i in range(1,len(attType)):\n",
    "            if(attType[i] == \"C\"):\n",
    "                attributeTypes.append(AttributeType.TYPE_CATEGORICAL)\n",
    "            else:\n",
    "                if (attType[i] == \"N\"):\n",
    "                    attributeTypes.append(AttributeType.TYPE_NUMERICAL)\n",
    "                else:\n",
    "                    print(\"ERROR: non recognized attribute type\", attType)\n",
    "        dataAttributes = [[0 for x in range(int(nbAttributes))] for y in range(int(nbSamples))]\n",
    "        dataClasses = [0 for y in range(int(nbSamples))]\n",
    "        nbLevels =  [0 for y in range(int(nbAttributes))]\n",
    "        \n",
    "        for s in range(0,nbSamples):\n",
    "            line = f.readline()\n",
    "            attributes = line.split(\" \")\n",
    "            for i in range(0,nbAttributes):\n",
    "                dataAttributes[s][i]=float(attributes[i])\n",
    "                if ((attributeTypes[i] == AttributeType.TYPE_CATEGORICAL) \n",
    "                    and (float(dataAttributes[s][i])+1 > nbLevels[i])):\n",
    "                    nbLevels[i] = float(dataAttributes[s][i])+1\n",
    "            dataClasses[s]=attributes[nbAttributes]\n",
    "        f.close()\n",
    "        \n",
    "        print(\"----- DATASET [\",datasetName,\"]\") # LOADED IN \" << clock()/ (double)CLOCKS_PER_SEC << \"(s)\" << std::endl;\n",
    "        print(\"----- NUMBER OF SAMPLES: \", nbSamples) \n",
    "        print(\"----- NUMBER OF ATTRIBUTES: \",nbAttributes)\n",
    "        print(\"----- NUMBER OF CLASSES: \",nbClasses)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Params(\"C:\\\\Users\\\\iannu\\\\Dropbox\\\\doutorado\\\\metaheuristics\\\\decision-tree\\\\Datasets\\\\p10.txt\",\"\",30,4,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self._nodeType = nodeType.NODE_NULL     # Node type\n",
    "        self._params = params                   # Access to the problem and dataset parameters\n",
    "        self._splitAttribute = -1               # Attribute to which the split is applied (filled through the greedy algorithm)\n",
    "        self._splitValue = -1.e30               # Threshold value for the split (for numerical attributes the left branch will be <= splitValue, for categorical will be == splitValue)\t\t\t\t\t\n",
    "        self._samples = []                      # Samples from the training set at this node\n",
    "        self._nbSamplesClass = [0 for x in params.nbClasses] # Number of samples of each class at this node (for each class)\n",
    "        self._nbSamplesNode = 0                 # Total number of samples in this node\n",
    "        self._majorityClass = -1                # Majority class in this node\n",
    "        self._maxSameClass = 0                  # Maximum number of elements of the same class in this node\n",
    "        self._entropy = -1.e30                  # Entropy in this node\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self._entropy = 0\n",
    "        for c in range(0,params.nbClasses):\n",
    "            if (nbSamplesClass[c]>0):\n",
    "                frac = nbSamplesClass[c]/nbSamplesNode\n",
    "                entropy -= frac * math.log(frac)\n",
    "                if (nbSamplesClass[c] > maxSameClass):\n",
    "                    maxSameClass = nbSamplesClass[c]\n",
    "                    majorityClass = c\n",
    "\n",
    "    def addSample(self, i):\n",
    "        self._samples.push_back(i)\n",
    "        nbSamplesClass[params.dataClasses[i]] += 1;\n",
    "        nbSamplesNode += 1\n",
    "\n",
    "class Solution:\n",
    "    tree = []\n",
    "    \n",
    "#     def printAndExport(self, fileName):\n",
    "#         nbMisclassifiedSamples = 0\n",
    "#         print(\"---------------------------------------- PRINTING SOLUTION ----------------------------------------\")\n",
    "#         for d in range(0,params.maxDepth):\n",
    "#             for i in range(2**d-1,2**(d+1)):\n",
    "#                 if (tree[i].nodeType == nodeType.NODE_INTERNAL):\n",
    "#                     print(i,\"A[\",tree[i].splitAttribute,\"]\",\"<=\" if params.attributeTypes[tree[i].splitAttribute] == typeNode.TYPE_NUMERICAL else \"=\",tree[i].splitValue,\")\")\n",
    "#                     else:\n",
    "#                         if (tree[i].nodeType == nodeType.NODE_LEAF):\n",
    "#                             misclass = tree[i].nbSamplesNode - tree[i].nbSamplesClass[tree[i].majorityClass]\n",
    "#                             nbMisclassifiedSamples += misclass\n",
    "                            \n",
    "\n",
    "# \t\t\t\t\tint misclass = tree[i].nbSamplesNode - tree[i].nbSamplesClass[tree[i].majorityClass];\n",
    "# \t\t\t\t\tnbMisclassifiedSamples += misclass;\n",
    "# \t\t\t\t\tstd::cout << \"(L\" << i << \",C\" << tree[i].majorityClass << \",\" << tree[i].nbSamplesClass[tree[i].majorityClass] << \",\" << misclass << \") \";\n",
    "# \t\t\t\t}\n",
    "# \t\t\t}\n",
    "# \t\t\tstd::cout << std::endl;\n",
    "# \t\t}\n",
    "# \t\tstd::cout << nbMisclassifiedSamples << \"/\" << params->nbSamples << \" MISCLASSIFIED SAMPLES\" << std::endl;\n",
    "# \t\tstd::cout << \"---------------------------------------------------------------------------------------------------\" << std::endl << std::endl;\n",
    "\n",
    "# \t\tstd::ofstream myfile;\n",
    "# \t\tmyfile.open(fileName.data());\n",
    "# \t\tif (myfile.is_open())\n",
    "# \t\t{\n",
    "# \t\t\tmyfile << \"TIME(s): \" << (params->endTime - params->startTime) / (double)CLOCKS_PER_SEC << std::endl;\n",
    "# \t\t\tmyfile << \"NB_SAMPLES: \" << params->nbSamples << std::endl;\n",
    "# \t\t\tmyfile << \"NB_MISCLASSIFIED: \" << nbMisclassifiedSamples << std::endl;\n",
    "# \t\t\tmyfile.close();\n",
    "# \t\t}\n",
    "# \t\telse\n",
    "# \t\t\tstd::cout << \"----- IMPOSSIBLE TO OPEN SOLUTION FILE: \" << params->pathToSolution << \" ----- \" << std::endl;\n",
    "# \t}\n",
    "    def __init__(self, params):\n",
    "        tree = [2**(params.maxDepth+1)-1]\n",
    "        for i in range(0,len(tree)):\n",
    "            tree[i] = Node(params)\n",
    "        \n",
    "        tree[0].nodeType = nodeType.NODE_LEAF\n",
    "        for i in range(0,params.nbSamples):\n",
    "            tree[0].addSample(i)\n",
    "        tree[0].evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-ac9366dcdcbc>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-ac9366dcdcbc>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    if (( level >= 0 params.maxDepth ) or ( nodeObj.maxSameClass ==nodeObj.nbSamplesNode )):\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def Greedy():\n",
    "    def __init__(self):\n",
    "        recursiveConstruction(0,0)\n",
    "    \n",
    "    def recursiveConstruction(self, node, level):\n",
    "        nodeObj = solution.tree[node]\n",
    "        # BASE CASES -- MAXIMUM LEVEL HAS BEEN ATTAINED OR ALL SAMPLES BELONG TO THE SAME CLASS\n",
    "        if (( level >= 0 params.maxDepth ) or ( nodeObj.maxSameClass ==nodeObj.nbSamplesNode )):\n",
    "            return\n",
    "        \n",
    "#         LOOK FOR A BEST SPLIT        \n",
    "        allIdentical = True\n",
    "        nbSamplesNode = nodeObj.nbSamplesNode\n",
    "        originalEntropy = nodeObj.entropy\n",
    "        bestInformationGain = -1.e30\n",
    "        bestSplitAttibute = -1\n",
    "        bestSplitThrehold = -1.e30\n",
    "        \n",
    "        for att in range(0, params.nbAttributes):\n",
    "            if (params.attributeTypes[att] == AttributeType.TYPE_NUMERICAL):\n",
    "#                 CASE 1) -- FIND SPLIT WITH BEST INFORMATION GAIN FOR NUMERICAL ATTRIBUTE c */\n",
    "\n",
    "#                 Define some data structures\n",
    "                orderedSamples = {}        #Order of the samples according to attribute c\n",
    "                attributeLevels = []       #Store the possible levels of this attribute among the samples (will allow to \"skip\" samples with equal attribute value)\n",
    "                for s in nodeObj.samples:\n",
    "                    orderedSamples[params.dataAttibutes[s][att]]=params.dataClasses[s]\n",
    "                    attributeLevels.append(params.dataAttibutes[s][att])\n",
    "                if (len(attributeLevels)<=1):\n",
    "                    continue\n",
    "                else:\n",
    "                    allIdentical = False\n",
    "                \n",
    "                #Initially all samples are on the right\n",
    "                \n",
    "                nbSamplesClassLeft = [0 for y in range(params.nbClasses)]\n",
    "                nbSamplesClassRight = nodeObj.nbSamplesClass\n",
    "                \n",
    "                indexSample = 0\n",
    "                filteredOrderedSamples = sorted({k : v for k,v in filter(lambda t: t[0] in [1, 3], orderedSamples.iteritems())})\n",
    "                for (attributeLevel in attributeLevels): #Go through all possible attribute values in increasing order\n",
    "#                   Iterate on all samples with this attributeValue and switch them to the left\n",
    "                    for (sampleKey, sampleValue in orderedSamples.items()):\n",
    "                        if (( indexSample>=nbSamplesNode ) or (orderedSamples[sampleKey] >= attributeValue + MY_EPSILON)):\n",
    "                            continue\n",
    "                            \n",
    "                        nbSamplesClassLeft[sampleValue]+=1\n",
    "                        nbSamplesClassRight[sampleValue]+=1\n",
    "                        indexSample+=1\n",
    "\n",
    "                    if (indexSample != nbSamplesNode):\n",
    "                        #Evaluate entropy of the two resulting sample sets\n",
    "                        entropyLeft = 0\n",
    "                        entropyRight = 0\n",
    "                        \n",
    "                        for (c in range(0,params.nbClasses)):\n",
    "                            #Remark that indexSample contains at this stage the number of samples in the left\n",
    "                            if (nbSamplesClassLeft[c]>0):\n",
    "                                fracLeft = nbSamplesClassesLeft[c] / indexSample\n",
    "                                entropyLeft = entropyLeft * math.log(entropyLeft)\n",
    "                            if (nbSamplesClassRight[c]>0):\n",
    "                                fracLeft = nbSamplesClassRight[c] / indexSample\n",
    "                                entropyRight = entropyRight * math.log(entropyRight)\n",
    "                            \n",
    "                            #Evaluate the information gain and store if this is the best option found until now\n",
    "                            informationGain = originalEntropy - (indexSample*entropyLeft + (nbSamplesNode - indexSample)*entropyRight)/nbSamplesNode\n",
    "                            \n",
    "                            if (informationGain > bestInformationGain):\n",
    "                                bestInformationGain = informationGain\n",
    "                                bestSplitAttribute = att\n",
    "                                bestSplitThrehold = attributeValue\n",
    "            else:\n",
    "                #CASE 2) -- FIND BEST SPLIT FOR CATEGORICAL ATTRIBUTE c \n",
    "                #Count for each level of attribute c and each class the number of samples\n",
    "                \n",
    "                nbSamplesLevel = [0 for y in range(params.nbLevels[att])]\n",
    "                nbSamplesClass = [0 for y in range(params.nbClasses)]\n",
    "                nbSamplesLevelClass = {}\n",
    "                \n",
    "                for k in params.nbLevels:\n",
    "                    nbSamplesLevelClass[k] = [0 for i in range(params.nbClasses)]\n",
    "                \n",
    "                for (s in nodeObj.samples):\n",
    "                    nbSamplesLevel[params->dataAttributes[s][att]]+=1\n",
    "                    nbSamplesClass[params.dataClasses[s]]+=1\n",
    "                    nbSamplesLevelClass[params.dataAttributes[s][att]][params.dataClasses[s]]+=1\n",
    "                                                \n",
    "                #Calculate information gain for a split at each possible level of attribute c\n",
    "                for level in range(0,params.nbLevels[att]):\n",
    "                    if (nbSamplesLevel[level] > 0 and nbSamplesLevel[level] < nbSamplesNode):\n",
    "                        #Evaluate entropy of the two resulting sample sets\n",
    "                        allIdentical = False\n",
    "                        entropyLevel = 0\n",
    "                        entropyOthers = 0\n",
    "                        for (c in range(0,params.nbClasses)):\n",
    "                            if (nbSamplesLevelClass[level][c] > 0):\n",
    "                                fracLevel = nbSamplesLevelClass[level][c] / nbSamplesLevel[level]\n",
    "                                entropyLevel -= fracLevel * math.log(fracLevel)\n",
    "                            if (nbSamplesClass[c] - nbSamplesLevelClass[level][c] > 0):\n",
    "                                fracOthers = (nbSamplesClass[c] - nbSamplesLevelClass[level][c]) / (nbSamplesNode - nbSamplesLevel[level])\n",
    "                                entropyOthers -= fracOthers * math.log(fracOthers)\n",
    "                        # Evaluate the information gain and store if this is the best option found until now\n",
    "                        informationGain = originalEntropy - (nbSamplesLevel[level] *entropyLevel + (nbSamplesNode - nbSamplesLevel[level])*entropyOthers) / nbSamplesNode \n",
    "                        if (informationGain > bestInformationGain):\n",
    "                            bestInformationGain = informationGain\n",
    "                            bestSplitAttribute = att\n",
    "                            bestSplitThrehold = level\n",
    "        # SPECIAL CASE TO HANDLE POSSIBLE CONTADICTIONS IN THE DATA \n",
    "        # (Situations where the same samples have different classes -- In this case no improving split can be found)\n",
    "\n",
    "        if (allIdentical) return\n",
    "\n",
    "        # APPLY THE SPLIT AND RECURSIVE CALL */\n",
    "        nodeObj.splitAttribute = bestSplitAttribute;nodeObj\n",
    "        nodeObj.splitValue = bestSplitThreholdnodeObj\n",
    "        nodeObj.nodeType = nodeType.NODE_INTERNAL\n",
    "        solution.tree[2*node+1].nodeType = nodeType.NODE_LEAF\n",
    "        solution.tree[2*node+2].nodeType = nodeType.NODE_LEAF\n",
    "        for (int s : nodeObj.samples):\n",
    "            if ((params.attributeTypes[bestSplitAttribute] == AttributeType.TYPE_NUMERICAL and params.dataAttributes[s][bestSplitAttribute] < bestSplitThrehold + MY_EPSILON) or (params->attributeTypes[bestSplitAttribute] == AttributeType.TYPE_CATEGORICAL and params.dataAttributes[s][bestSplitAttribute] < bestSplitThrehold + MY_EPSILON and params.dataAttributes[s][bestSplitAttribute] > bestSplitThrehold - MY_EPSILON)):\n",
    "                solution.tree[2*node+1].addSample(s)\n",
    "            else:\n",
    "                solution.tree[2*node+2].addSample(s)\n",
    "        solution.tree[2*node+1].evaluate() # Setting all other data structures\n",
    "        solution.tree[2*node+2].evaluate() # Setting all other data structures\n",
    "        recursiveConstruction(2*node+1,level+1) # Recursive call\n",
    "        recursiveConstruction(2*node+2,level+1) # Recursive call\n",
    "                    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
